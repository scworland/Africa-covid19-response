---
title: "Scrapping Africaâ€™s Response to COVID-19 from Twitter"
author: "Alex Baker"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  github_document:
    html_preview: false
    toc: true
    toc_depth: 2
---

```{r setup, echo=FALSE, include=FALSE}
knitr::opts_knit$set(root.dir = normalizePath("~/Documents/projects/Africa-covid19-response")) 
library(dplyr)
library(rtweet)
library(tidyverse)
theme_set(theme_bw(base_size=20))
```

Load csv of twitter handles,

```{r}
news_sites = read_csv('data/africa_news_sites_4_6_2020.csv', na = c("null",""," ")) %>%
  set_names(c("news_url", "twitter_handle"))

print(news_sites)
```

To search the handles we need to strip the "@", look up user, and collect the data,

```{r, warning=FALSE, message=FALSE, eval=FALSE}
# clean and prepare handles
handles = news_sites %>%
  mutate(twitter_handle = gsub("@|\n", "", twitter_handle)) %>% # remove @
  drop_na(twitter_handle) %>% # remove rows without handles
  select(twitter_handle) # extract handle column

# search twitter posts from handles
# twitter_timeline = get_timelines(handles, n=3200) 

# clean post for analysis 
twitter_posts = twitter_timeline %>%
  mutate(text=tolower(text)) %>%
  filter(stringr::str_detect(text, 'corona|coronavirus|virus|novel|covid|covid-19|pandemic')) %>%
  select(created_at,screen_name,text,favorite_count,retweet_count,
         hashtags, linked_url=urls_expanded_url) %>% # select specific columns
  unnest(linked_url) %>% # convert url from list to column
  mutate(hashtags = set_names(hashtags, screen_name), # add names to list
         hashtags = map(hashtags, str_c, collapse=", ")) %>% # create char string
  unnest(hashtags) # convert list to column

# write_csv(twitter_posts %>% slice(1:100),"data/twitter_url_sample.csv")
```

```{r}
# identify new twitter handles
known_handles = handles %>%
                  anti_join(known_handles, by='twitter_handle') %>%
                  mutate(added_at=Sys.time(), is_new_handle=TRUE) %>%
                  full_join(known_handles, by='twitter_handle')
```

```{r}
known_handles = data.frame(known_handles, stringsAsFactors = FALSE)
```

# Creating Metdata Files Required to Run Scapper

```{r}
# known_handles$is_new_handle = TRUE
write.csv(data.frame(twitter_handle="TimesofAlgeria", added_at="2020-04-11 12:37:16", is_new_handle=TRUE, stringsAsFactors=FALSE), file='data/data_dump/accounts.csv', row.names=FALSE)
write.csv(data.frame(file_name="", file_path="", created_at="", stringsAsFactors=FALSE), file='data/data_dump/data_dump_meta.csv', row.names=FALSE)
```

```{r}
accounts = read.csv(file='data/data_dump/accounts.csv')
accounts
```

```{r}
accounts$is_new_handle[accounts$twitter_handle == 'TimesofAlgeria'] = FALSE
```

```{r}
new_accounts = accounts %>% filter(is_new_handle == TRUE) %>% pull(twitter_handle)
known_accounts = accounts %>% filter(is_new_handle == FALSE) %>% pull(twitter_handle)
```

```{r}
print (new_accounts)
```

# Creating Fetch Twitter Data API Pipeline

```{r}
twitter_scapper = function(inputted_accounts_fp){
  # wrap function to scrap tweets from twitter that discuss covid-19 in Africa
  
  # loading known twitter accounts that discuss covid-19 in Africa and data dump metadata
  accounts = read.csv('../data/data_dump/accounts.csv', stringsAsFactors=FALSE)
  data_dump_meta = read.csv('../data/data_dump/data_dump_meta.csv', stringsAsFactors=FALSE)
  
  # reading Jason and Collaborators inputted twitter account file
  # and identifying newly added twitter accounts
  inputted_accounts = read.csv(inputted_accounts_fp)
  cleaned_inputs = clean_inputted_accounts(inputted_accounts)
  accounts = identify_new_accounts(cleaned_inputs, accounts)
  
  # fetch tweets from twitter accounts and extract tweets that discuss covid-19
  tweets = fetch_tweets(accounts)
  tweets = extract_covid19_tweets(tweets)
  
  # save tweets to data dump directory.
  current_datetime = format(Sys.time(), "%Y_%m_%d_%I_%M_%p")
  tweets_filename = paste0("covid19_africa_raw_tweets_", current_datetime, ".csv")
  tweets_fp = paste0('../data/data_dump/', tweets_filename)
  write.csv(tweets, tweets_fp)
  
  # update and save data dump metadata
  new_data_dump = data.frame(file_name=tweets_filename, file_path=tweets_fp, created_at=current_datetime)
  data_dump_meta = rbind(data_dump_meta, new_data_dump)
  write.csv(data_dump_meta, '../data/data_dump/data_dump_meta.csv', row.names=FALSE)
  
  # update and save accounts dataframe
  accounts$is_new_handles = FALSE
  write.csv(accounts, '../data/data_dump/accounts.csv', row.names=FALSE)
}

clean_inputted_accounts = function(inputted_accounts){
  # clean the inputted accounts data for downstream use.
  cleaned_input = inputted_accounts %>%
    mutate(twitter_handle = str_remove(twitter_handle,"@|\n")) %>% # remove @
    drop_na(twitter_handle) %>% # remove rows without handles
    select(twitter_handle) # extract handle column
  
  return (cleaned_input)
}

identify_new_accounts = function(cleaned_input, accounts){
  # merge the cleaned input that do not have an entry in the accounts
  # dataframe. Allowing us know when new accounts have been added.
  accounts = cleaned_input %>%
                  anti_join(accounts, by='twitter_handle') %>%
                  mutate(added_at=Sys.time(), 
                         is_new_handle=TRUE, 
                         most_recent_tweet_id=NULL) %>%
                  full_join(accounts)
  
  return (accounts)
}

fetch_tweets = function(accounts){
  # fetch acount timeline tweets from twitter api for a given user
  timelines = data.frame()
  new_accounts = subset(accounts$twitter_handle, accounts$is_new_handle == TRUE)
  known_accounts = subset(accounts, accounts$is_new_handle == FALSE)
  
  if (nrow(new_accounts) > 0){
    # if there are new users fetch as many recent tweets as possible
    new_timelines = get_timelines(new_accounts, n=3200) 
    timelines = rbind(timelines, new_timelines)
  }
  
  if (nrow(known_accounts) > 0) {
    # fetch all tweets from the most recent tweet we have onwards
     known_timelines = fetch_most_recent_tweets(known_accounts) 
     timelines = rbind(timelines, known_timelines)
  }
  
  return (timelines)
}

fetch_most_recent_tweets = function(known_accounts){
   # fetch all tweets from the most recent tweet we have onwards
  most_recent_tweets = data.frame()

  # loop through each known account and fetch the tweets from most recent tweet onwards
  # By for looping we can specify the since_id parameter of the twitter api. the since_id
  # tells the api to fetch all tweets since a given twee
  for (row in 1:nrow(known_accounts)){
    current_account = test[row, ]
    most_recent_tweet_id = current_account$most_recent_tweet_id
    account_twitter_handle = current_account$twitter_handle
    
    fetched_data = get_timeline(account_twitter_handle, since_id=most_recent_tweet_id)
    most_recent_tweets = rbind(most_recent_tweets, fetched_data)
  }
  
  return (most_recent_tweets)
}

extract_covid19_tweets = function(tweets){
  # searching for all tweets that discuss covid-19 via a regex key word search
  
  covid_posts = tweets %>%
    mutate(text=tolower(text)) %>%
    filter(stringr::str_detect(text, 'corona|coronavirus|virus|novel|covid|covid-19|pandemic')) %>%
    select(screen_name,created_at,id,location,text,favorite_count,
           retweet_count,hashtags,linked_url=urls_expanded_url) %>% # select specific columns
    unnest(linked_url) %>% # convert url from list to column
    mutate(hashtags = set_names(hashtags, screen_name), # add names to list
           hashtags = map(hashtags, str_c, collapse=", ")) %>% # create char string
    unnest(hashtags) # convert list to column
  
  return (covid_posts)
}
```

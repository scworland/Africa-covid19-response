---
title: "ETL of Raw Twitter Data"
author: "Alex Baker"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  github_document:
    html_preview: false
    toc: true
    toc_depth: 2
---

# Load Packages and Scrapped Tweets.

```{r setup, echo=FALSE, include=FALSE}
knitr::opts_knit$set(root.dir = normalizePath("~/Documents/projects/Africa-covid19-response")) 

library(tm)
library(umap)
library(cld2)
library(rvest)
library(rtweet)
library(tidytext)
library(textstem)
library(text2vec)
library(tidyverse)

theme_set(theme_bw(base_size=20))
twitter_posts <- read_csv("../data/twitter_url_complete.csv")
stop_words = stopwords('en')
```

# Extraction of Data from Tweets

One thing i need to do is make sure i can still track the 

## Cleaning Twitter Text Data

Here's an initial ETL pipeline to clean and prep text data from tweets. Pipeline starts off by removing embedded urls and new line characters from tweets. Then moves on to strip whitespace and then remove punctuations from the text. Next we remove all empty tweets (tweets sharing links) and detect what language was used to write the tweet. Using the CLD2 model (which performs better than CLD3) I detect all tweets writen in english and remove all non-english tweets. Finally, I normalize the words by lemmatize the text. Lemmatization is a text normalization method that takes a word and looks up it's lemma (root word) in the dictionary. It's slowerly than stemming but tends to 
provide better results and more context.

The text that i am going to vectorize off of is stored in the normalized column. I created this column so i can store the original text from the tweet (further thought is required in how to organize the processed data).

```{r}
# stop word regular expression
covid_words = c('corona', 'coronavirus','virus', 'novel','covid', 'covid19', 'covid-19','pandemic', 'will')
stop_words = c(stopwords('en'), covid_words)
stopwords_regex = paste(stop_words, collapse = '\\b|\\b')
stopwords_regex = paste0('\\b', stopwords_regex, '\\b')
```

```{r}
cleaned_twitter_data = twitter_posts %>%
                        mutate(text = gsub(" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", "", text), # remove urls embedded into the tweet text
                              text = gsub("[\r\n']", "", text), # remove new line characters and replace them with white space
                              text = gsub("[[:punct:]]+", "", text),
                              text = removePunctuation(text), # remove punctuation
                              normalized = str_replace_all(text, stopwords_regex, ""), # remove stop words from tweets
                              normalized = stripWhitespace(normalized), # remove unnecessary white space from tweets text
                              normalized = str_to_lower(normalized)) %>%  # then remove/strip unnecessary white space
                        filter(text != "" | normalized != "") %>% # remove empty tweets
                        mutate(language_used = detect_language(text)) %>% # detect language used in tweets text using cld2 model by google
                        filter(language_used == "en") %>% # remove all non-english tweets
                        mutate(normalized = lemmatize_strings(normalized), # finally normalize words by lemmantization.
                               id = paste0(created_at, '_', screen_name)) # creating a guid by combining datetime and twitter handle
```

```{r}
write.csv(cleaned_twitter_data, file='data/cleaned_twitter_data.csv')
head(cleaned_twitter_data)
```

## Vectorizing Twitter Text

```{r}
it_tweets = itoken(cleaned_twitter_data$normalized, 
                   tokenizer = word_tokenizer,
                   ids = cleaned_twitter_data$id)

tweet_vocab = create_vocabulary(it_tweets)
tweet_vectorizer = vocab_vectorizer(tweet_vocab)
tweet_dtm = create_dtm(it_tweets, tweet_vectorizer)
```

```{r}
lda = LDA$new(n_topics=10, doc_topic_prior=0.1, topic_word_prior=0.01)
lda_topic_embedding = lda$fit_transform(x=tweet_dtm, n_iter=1000, convergence_tol=0.001, n_check_convergence=25)
```

```{r}
lda_umap = umap(lda_topic_embedding)
```

```{r}
topic_key_words = c()

for (i in 1:ncol(lda_topic_embedding)){
   key_words = paste(lda$get_top_words(n=5)[, i], collapse="_")
   topic_key_words = c(topic_key_words, key_words)
}

colnames(lda_topic_embedding) = topic_key_words
lead_topic = colnames(lda_topic_embedding)[apply(lda_topic_embedding, 1, which.max)]
```

```{r topics_umaps, fig.width=11, fig.height=11}
tibble(x=lda_umap$layout[,1], y = lda_umap$layout[,2], lead_topic=lead_topic) %>%
  ggplot(aes(x=x, y=y, colour=lead_topic)) +
  geom_point()
```

# Functionizing Code

```{r}

clean_data = function(raw_data){
  # cleaning text from tweets pulled from twitter.
  cleaned_data = raw_data %>%
                        mutate(text = gsub(" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", "", text), # remove urls embedded into the tweet text
                              text = gsub("[\r\n']", "", text), # remove new line characters and replace them with white space
                              text = gsub("[[:punct:]]+", "", text),
                              text = removePunctuation(text), # remove punctuation
                              normalized = str_replace_all(text, stopwords_regex, ""), # remove stop words from tweets
                              normalized = stripWhitespace(normalized), # remove unnecessary white space from tweets text
                              normalized = str_to_lower(normalized)) %>%  # then remove/strip unnecessary white space
                        filter(text != "" | normalized != "") %>% # remove empty tweets
                        mutate(language_used = detect_language(text)) %>% # detect language used in tweets text using cld2 model by google
                        filter(language_used == "en") %>% # remove all non-english tweets
                        mutate(normalized = lemmatize_strings(normalized), # finally normalize words by lemmantization.
                               id = paste0(created_at, '_', screen_name)) # creating a guid by combining datetime and twitter handle
  
  # cache cleaned tweets.
  write.csv(cleaned_data, file=paste0('data/cleaned_twitter_data_', Sys.time()))
  return (cleaned_data)
}


create_dtm = function(cleaned_data){
  # generate a document term matrix. First i tokenize the tweets text and then create a document term matrix.
  it = itoken(cleaned_data$normalized, 
             tokenizer = word_tokenizer,
             ids = cleaned_data$id)

  vocab = create_vocabulary(it)
  vectorizer = vocab_vectorizer(vocab)
  dtm = create_dtm(it, vectorizer)
  
  return (dtm)
}

new_topic_model = function(){
  # create a new LDA model.
  lda = LDA$new(n_topics=10, doc_topic_prior=0.1, topic_word_prior=0.01)
  return(lda)
}

topic_embedding = function(dtm, lda){
  # generate topic embedding from document term matrix using LDA
  embedding = lda$fit_transform(x=dtm, n_iter=1000, convergence_tol=0.001, n_check_convergence=25)
  return (embedding)
}

fetch_topic_key_words = function(embedding, lda){
  # naming the topic columns by there top 5 key words.
  topic_key_words = c()
  
  for (i in 1:ncol(embedding)){
       key_words = paste(lda$get_top_words(n=5)[, i], collapse="_")
       topic_key_words = c(topic_key_words, key_words)
    }
    
    colnames(embedding) = topic_key_words
    return (embedding)
}

visualize_topic_embeddings = function(embedding){
  # visualizing all topic embeddings via umap projections
  topic_key_words = colnames(embedding)[apply(embedding, 1, which.max)]
  embedding_umap = umap(embedding)
  layout = embedding_umap$layout
  
  tibble(x=layout[,1], y=layout[,2], topic_key_words=topic_key_words) %>%
    ggplot(aes(x=x, y=y, colour=topic_key_words)) +
    geom_point()
}

```
